{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75e2ef28-594f-4c18-9d22-c6b8cd40ead2",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n",
      "Google API Key exists and begins AIzaSyDo\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6541d58e-2297-4de1-b1f7-77da1b98b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e16839b5-c03b-4d9d-add6-87a0f6f37575",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e97227-f162-4d1a-a0b2-345ff248cbe7",
   "metadata": {},
   "source": [
    "# Please read this! A change from the video:\n",
    "\n",
    "In the video, I explain how we now need to write a function called:\n",
    "\n",
    "`chat(message, history)`\n",
    "\n",
    "Which expects to receive `history` in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "But Gradio has been upgraded! Now it will pass in `history` in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function `chat(message, history)` where:  \n",
    "**message** is the prompt to use  \n",
    "**history** is the past conversation, in OpenAI format  \n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler than in my video - we can easily create this function that calls OpenAI\n",
    "# It's now just 1 line of code to prepare the input to OpenAI!\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1334422a-808f-4147-9c4c-57d63d9780d0",
   "metadata": {},
   "source": [
    "## And then enter Gradio's magic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0866ca56-100a-44ab-8bd0-1568feaf6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales evemt.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5be3ec-c26c-42bc-ac16-c39d369883f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f0ffa-55c8-4152-b451-945021676837",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message += \"\\nIf the customer asks for shoes, you should respond that shoes are not on sale today, \\\n",
    "but remind the customer to look at hats!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c602a8dd-2df7-4eb7-b539-4e01865a6351",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a987a66-1061-46d6-a83a-a30859dc88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed a bug in this function brilliantly identified by student Gabor M.!\n",
    "# I've also improved the structure of this function\n",
    "\n",
    "def chat(message, history):\n",
    "\n",
    "    relevant_system_message = system_message\n",
    "    if 'belt' in message:\n",
    "        relevant_system_message += \" The store does not sell belts; if you are asked for belts, be sure to point out other items on sale.\"\n",
    "    \n",
    "    messages = [{\"role\": \"system\", \"content\": relevant_system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20570de2-eaad-42cc-a92c-c779d71b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a57ee0-b945-48a7-a024-01b56a5d4b3e",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business Applications</h2>\n",
    "            <span style=\"color:#181;\">Conversational Assistants are of course a hugely common use case for Gen AI, and the latest frontier models are remarkably good at nuanced conversation. And Gradio makes it easy to have a user interface. Another crucial skill we covered is how to use prompting to provide context, information and examples.\n",
    "<br/><br/>\n",
    "Consider how you could apply an AI Assistant to your business, and make yourself a prototype. Use the system prompt to give context on your business, and set the tone for the LLM.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66707979-445d-4af9-b419-6f87c9daf6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f5114ec-1fda-427c-8796-33a10ab8bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Gemini (do this ONCE outside the chat function)\n",
    "try:\n",
    "    genai.configure(api_key=google_api_key)  # Replace with your actual API key\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Gemini: {e}\")\n",
    "    model = None  # Set model to None to indicate failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9ce6bed-888f-487c-b1fb-4bec3343fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example prompt\n",
    "system_prompt = \"You are a freindly and helpful assistant. You are responsible to answer the questions to our client that \\\n",
    "visit our web site. You only responsible to answer the following types of product:\\\n",
    "1. printer\\\n",
    "2. web camera\\\n",
    "3. laptop\\\n",
    "if visitor ask question out of those scope, you can simplely answer, I dont know the answer.\\\n",
    "you need to provide introduce and suggestion to win purchase from vistor.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73f3b318-7d41-4321-8021-7ef63ed9ef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intelligence (AI) is a broad field encompassing many techniques, but at its core, it aims to create systems that can perform tasks that typically require human intelligence.  There's no single \"how it works\" because different AI approaches use different methods.  However, we can break it down into key concepts:\n",
      "\n",
      "**1. Data is King:**  AI systems learn from data. The more relevant and high-quality data they're trained on, the better they perform. This data can be anything from images and text to sensor readings and financial transactions.\n",
      "\n",
      "**2. Algorithms are the Recipes:** Algorithms are sets of instructions that tell the AI system how to process and learn from the data.  Different algorithms are suited for different tasks.  Some common types include:\n",
      "\n",
      "* **Machine Learning (ML):** This is a subfield of AI where systems learn from data without explicit programming.  Instead of being given rules, they identify patterns and relationships within the data to make predictions or decisions.  Examples include:\n",
      "    * **Supervised Learning:** The algorithm is trained on labeled data (data where the correct answer is provided).  For example, showing a computer thousands of pictures of cats and dogs, labeled as such, to teach it to identify them.\n",
      "    * **Unsupervised Learning:** The algorithm is trained on unlabeled data and tries to find structure or patterns on its own.  For example, grouping customers based on their purchasing behavior without pre-defined customer segments.\n",
      "    * **Reinforcement Learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones.  This is often used in robotics and game playing.\n",
      "\n",
      "* **Deep Learning (DL):** A subset of machine learning that uses artificial neural networks with multiple layers (hence \"deep\"). These networks are inspired by the structure and function of the human brain.  Deep learning excels at tasks involving complex patterns, such as image recognition, natural language processing, and speech recognition.\n",
      "\n",
      "**3. Models are the Representations:**  After training on data using an algorithm, the AI system creates a model. This model is a mathematical representation of the patterns and relationships learned from the data.  It's this model that's used to make predictions or decisions on new, unseen data.\n",
      "\n",
      "**4. Inference is the Application:** Once a model is trained, it can be used for inference â€“ making predictions or taking actions based on new input data.  For example, a trained image recognition model can identify objects in a new image.\n",
      "\n",
      "**Simplified Analogy:** Imagine teaching a child to identify different fruits.  You show them many examples of apples, oranges, and bananas, labeling each one.  This is like supervised learning.  The child learns to recognize the patterns (shape, color, etc.) that distinguish each fruit.  This learned pattern is their \"model.\"  Then, when you show them a new fruit, they can use their model to identify it (inference).\n",
      "\n",
      "**In short:** AI systems learn from data using algorithms to create models that can then be used to make predictions or decisions on new data. The complexity and specific techniques vary greatly depending on the task and the type of AI being used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(\"Explain how AI works\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc5fc92c-28d2-4827-be68-6583eea709a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history=[]):\n",
    "    if model is None:\n",
    "        yield \"Gemini is not initialized. Please check your API key and network connection.\"\n",
    "        return\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"parts\": [{\"text\": system_message}]}] + \\\n",
    "               [{\"role\": msg[\"role\"], \"parts\": [{\"text\": msg[\"content\"]}]} for msg in history] + \\\n",
    "               [{\"role\": \"user\", \"parts\": [{\"text\": message}]}]\n",
    "    try:\n",
    "        response = model.generate_content(messages)\n",
    "        # Check for errors in the response\n",
    "        # if response.prompt_feedback and response.prompt_feedback.block_reason:\n",
    "        #     yield f\"Response blocked due to: {response.prompt_feedback.block_reason}\"\n",
    "        #     return\n",
    "\n",
    "        # Extract the text from the response\n",
    "        if response.parts:\n",
    "            text_response = \"\".join([part.text for part in response.parts])\n",
    "            yield text_response\n",
    "        else:\n",
    "            yield \"No text content in the response.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        yield f\"An error occurred during generation: {e}\"\n",
    "\n",
    "# # Example usage (outside a web framework):\n",
    "# history = []\n",
    "# user_message = \"What is the capital of France?\"\n",
    "# for bot_response in chat(user_message, history):\n",
    "#     print(f\"Bot: {bot_response}\")\n",
    "#     history.append({\"role\": \"user\", \"content\": user_message}) # Important: Add user message to history\n",
    "#     history.append({\"role\": \"model\", \"content\": bot_response}) # And bot response\n",
    "\n",
    "# user_message = \"What is the weather like there?\"\n",
    "# for bot_response in chat(user_message, history):\n",
    "#     print(f\"Bot: {bot_response}\")\n",
    "#     history.append({\"role\": \"user\", \"content\": user_message})\n",
    "#     history.append({\"role\": \"model\", \"content\": bot_response})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b726c86-8e43-4ae5-8adb-dcb37a7d61f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7869\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7869/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe2f4c-7d1f-4f0f-8a58-e9e807022b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
