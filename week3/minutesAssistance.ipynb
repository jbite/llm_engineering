{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91390160-a44f-4d55-826c-0f66f38662b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate openai httpx==0.27.2 gradio diffusers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a2359f-6087-4c8a-811b-9386ee22def5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from dotenv import load_dotenv\n",
    "from diffusers import DiffusionPipeline\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from huggingface_hub import login\n",
    "import google.generativeai as genai\n",
    "from google.generativeai import types\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, pipeline\n",
    "from datasets import load_dataset\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a8c49fc-05ff-4418-b968-f1123989efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "AUDIO_MODEL = \"gemini-1.5-flash\"\n",
    "LLAMA = \"meta-llama/Meta-Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56cdce86-493f-41a3-9d28-717934174407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv()\n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'your-key-if-not-using-env')\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31b1118e-63a5-43f9-a7d4-8e6604a1c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create meeting minutes assistance\n",
    "class MeetingAssistance:\n",
    "  def __init__(self, audio_file, prompt):\n",
    "    self.audio_file = audio_file\n",
    "    self.prompt = prompt\n",
    "    self.messages = []\n",
    "    self.sumText = \"\"\n",
    "    self.image = None\n",
    "    self.sumAudio = None\n",
    "\n",
    "  def transcribe_file_with_auto_punctuation(self):\n",
    "    # create an instance of generative agent\n",
    "    client = genai.Client(api_key=google_api_key)\n",
    "    with open(self.audio_file, 'rb') as f:\n",
    "      image_bytes = f.read()\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "      model=AUDIO_MODEL,\n",
    "      contents=[\n",
    "        \"transcribr the audio file that provided\",# prompt\n",
    "        types.Part.from_bytes(\n",
    "          data=image_bytes,\n",
    "          mime_type='audio/mp3',\n",
    "        )\n",
    "      ]\n",
    "    )\n",
    "    # system_message = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "    user_prompt = f\"Below is an extract transcript of a Denver council meeting. \\\n",
    "    Please write minutes in markdown, including a summary with attendees, location and date; \\\n",
    "    discussion points; takeaways; and action items with owners.\\n{response.text}\"\n",
    "    self.messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "      ]\n",
    "\n",
    "  def summarize(self):\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_use_double_quant=True,\n",
    "      bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "      bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LLAMA)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer.apply_chat_template(self.messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "    model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config=quant_config)\n",
    "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "\n",
    "    self.sumText = tokenizer.decode(outputs[0])\n",
    "    \n",
    "  def text_to_audio(self):\n",
    "    synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\", device='cuda')\n",
    "    speaker_embedding = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "\n",
    "    # Split text into smaller chunks\n",
    "    chunk_size = 500  # Adjust as needed\n",
    "    chunks = [textToSpeech[i:i+chunk_size] for i in range(0, len(textToSpeech), chunk_size)]\n",
    "\n",
    "    speech_chunks = []\n",
    "    for chunk in chunks:\n",
    "        speech = synthesiser(chunk, forward_params={\"speaker_embeddings\": speaker_embedding})\n",
    "        speech_chunks.append(speech[\"audio\"])\n",
    "\n",
    "    # Concatenate audio chunks (if necessary)\n",
    "    combined_speech = torch.cat(speech_chunks)\n",
    "\n",
    "    sf.write(\"speech.wav\", combined_speech, samplerate=speech[\"sampling_rate\"])\n",
    "    self.sumAudio = Audio(\"speech.wav\")\n",
    "\n",
    "  def text_to_image(self):\n",
    "    # Generate a prompt for image\n",
    "    generator = pipeline(\"text-generation\", device=\"cuda\")\n",
    "    result = generator(f\"extract the key word for the following article.{self.sumText}\")\n",
    "\n",
    "    # Image Generation\n",
    "    image_gen = DiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-diffusion-2\",\n",
    "        torch_dtype=torch.float16,\n",
    "        use_safetensors=True,\n",
    "        variant=\"fp16\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "    text = result[0]['generated_text']\n",
    "    print(text)\n",
    "    self.image = image_gen(prompt=text).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c0cd99f-1dad-4fcd-b5cf-79199cbe4d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76159bbec8fb40e19cec9fd525c61244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\checfeng\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\checfeng\\.cache\\huggingface\\hub\\models--meta-llama--Meta-Llama-3.1-8B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6763a2ce0d704e44b68458d0b7073bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56a02c67384599808b7e9dd37dab47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m audio_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mchecfeng\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mOneDrive - Cisco\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mLLM\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mllm_engineering\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mweek3\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdenver_extract.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m assistance \u001b[38;5;241m=\u001b[39m MeetingAssistance(audio_filename,prompt)\n\u001b[1;32m----> 4\u001b[0m \u001b[43massistance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# assistance.text_to_audio()\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# assistance.text_to_image()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 46\u001b[0m, in \u001b[0;36mMeetingAssistance.summarize\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(LLAMA)\n\u001b[0;32m     45\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n\u001b[1;32m---> 46\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_chat_template\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(tokenizer)\n\u001b[0;32m     48\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(LLAMA, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, quantization_config\u001b[38;5;241m=\u001b[39mquant_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1632\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.apply_chat_template\u001b[1;34m(self, conversation, tools, documents, chat_template, add_generation_prompt, continue_final_message, tokenize, padding, truncation, max_length, return_tensors, return_dict, return_assistant_tokens_mask, tokenizer_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m \u001b[38;5;66;03m# Compilation function uses a cache to avoid recompiling the same template\u001b[39;00m\n\u001b[0;32m   1629\u001b[0m compiled_template \u001b[38;5;241m=\u001b[39m _compile_jinja_template(chat_template)\n\u001b[0;32m   1631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(conversation, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m-> 1632\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[43mconversation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(conversation[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1633\u001b[0m ):\n\u001b[0;32m   1634\u001b[0m     conversations \u001b[38;5;241m=\u001b[39m conversation\n\u001b[0;32m   1635\u001b[0m     is_batched \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "prompt = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "audio_filename = \"C:\\\\Users\\\\checfeng\\\\OneDrive - Cisco\\\\Desktop\\\\LLM\\\\llm_engineering\\\\week3\\\\denver_extract.mp3\"\n",
    "assistance = MeetingAssistance(audio_filename,prompt)\n",
    "assistance.summarize()\n",
    "# assistance.text_to_audio()\n",
    "# assistance.text_to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c027fc4-e182-44c9-b363-8fe31543c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MinutesSummarizer(audio_file, prompt):\n",
    "    assistance = MeetingAssistance(audio_file, prompt)\n",
    "    assistance.transcribe_file_with_auto_punctuation()\n",
    "    assistance.summarize()\n",
    "    assistance.text_to_audio()\n",
    "    assistance.text_to_image()\n",
    "    return assistance.sumText, assistance.image, assistance.sumAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f78d3-752d-41bd-a3eb-5b85a597dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba077ff3-5974-4759-b73c-cf3eb350dd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultPrompt = \"You are an assistant that produces minutes of meetings from transcripts, with summary, key discussion points, takeaways and action items with owners, in markdown.\"\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=MinutesSummarizer,\n",
    "    inputs=[gr.File(label=\"Upload Audio\"), gr.Textbox(value=defaultPrompt, label=\"Prompt(Optional)\", lines=3,)],\n",
    "    outputs=[gr.Textbox(label=\"Minutes\"), gr.Image(label=\"Image\"), gr.Audio(label=\"Audio\")],\n",
    "    title=\"Audio Transcription with Auto Punctuation\",\n",
    "    description=\"Upload an audio file and get a transcription with automatic punctuation\"\n",
    ")\n",
    "\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
